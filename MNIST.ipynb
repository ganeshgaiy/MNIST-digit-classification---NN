{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1rAJK13oR6vVzujfQMlF2DnCs0twGqaAk",
      "authorship_tag": "ABX9TyNpcbfDVCNrvTrg2V5MLFyN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshgaiy/MNIST-digit-classification---NN/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchshow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoxWxfK_I3DV",
        "outputId": "a45011a8-2e85-4a53-c3e8-388f3c2ce296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchshow in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchshow) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torchshow) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torchshow) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->torchshow) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqKrj0-kbcgo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchshow as ts"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APfd9Jlpgb6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = torch.tensor(8)"
      ],
      "metadata": {
        "id": "JGf_HLF4bnI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalar.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obGqUtJgeAP3",
        "outputId": "16f59dfb-3d25-42b3-fc30-9f2b4528ab3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kNGRa4hM4Fe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = torch.tensor([7,8])"
      ],
      "metadata": {
        "id": "3xFMTeOie_kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B-5Hy6T5ODG",
        "outputId": "f4d3fe79-4ebb-42bc-9d8a-dbd2a1a5e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwTbrLICFn98",
        "outputId": "5ba6bf74-0dd9-43af-d8e2-dd5f75d8d8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr = torch.tensor([[[7,8,3],[8,9,3],[1,2,3]]])"
      ],
      "metadata": {
        "id": "ny_An3XzFpQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRoDTOd3Fw9d",
        "outputId": "91e0f0d4-eb92-4886-8b1b-de1c63c9ef1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeSE9u_sF2P_",
        "outputId": "75087136-dd56-4597-931f-caf83a09669f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tnsr is a 2 dimensional tensor with 2 columns"
      ],
      "metadata": {
        "id": "C3tbZwQ7Gnrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1 = torch.tensor([[[0,0],[9,10],[11,12],[13,14]]])"
      ],
      "metadata": {
        "id": "u804j4tiF3f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGEQ8v--HFyh",
        "outputId": "e00557f7-df9d-420c-f394-5a8c1e81a986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  0],\n",
              "         [ 9, 10],\n",
              "         [11, 12],\n",
              "         [13, 14]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bOc7knqITtR",
        "outputId": "fa6666e5-82d2-474f-fdec-8bdd2ba74cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nuUtxaAIjrr",
        "outputId": "7350c048-4089-4101-d5a0-11a24befa4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(1,4,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_H6TOiJAzh",
        "outputId": "65b62652-2cae-4793-85d1-c4f935563f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3070, 0.0809],\n",
              "         [0.6897, 0.6298],\n",
              "         [0.8354, 0.3676],\n",
              "         [0.1127, 0.8343]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(1,2,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uhBn9gLTRba",
        "outputId": "523af230-877f-47d7-ba71-edaae8b74792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0.],\n",
              "         [0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(1,2,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGAZKY6sUkou",
        "outputId": "610ffee0-52c9-426f-95af-a1dc441ec4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1.],\n",
              "         [1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(1,10)"
      ],
      "metadata": {
        "id": "Cs6s39uoUrig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nRse2MqbcnbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q43CTzoMV9a4",
        "outputId": "e0b2ae83-60ba-4369-81fc-f7af602f3bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKOzU6A3WBBj",
        "outputId": "730b7c3d-541d-4292-9c2b-ad26ffdffd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT2YMy5JWCT-",
        "outputId": "c2d6a360-0ec0-4610-94ac-039199641c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lCli_EVWjbl",
        "outputId": "ca790b71-8a07-4aa5-97a3-e5fed25b83eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1+10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNV_IthbZ4W8",
        "outputId": "ce637c83-61b6-4a61-d22c-0310e0115e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[10, 10],\n",
              "         [19, 20],\n",
              "         [21, 22],\n",
              "         [23, 24]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.type(torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97qIBkU6cl5g",
        "outputId": "1ec7e8f7-5d7a-4e2d-fe59-84def9995068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3iHxIEheoiz",
        "outputId": "80b2a2a4-2962-488b-9c0d-61356e785702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyvNtZQGfDGC",
        "outputId": "bfbb5c03-74e9-4ee3-dc5a-93dc3aa1463f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb8BuS9LfEv5",
        "outputId": "94d12e0d-9477-4ad0-a217-54d0e29fd658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(45)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXtmxHAhfF-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.type(torch.float32).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVoPRo49fKUa",
        "outputId": "da017d2a-a1a0-4913-898f-c7d0c6f57b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2VlUPmOfR_1",
        "outputId": "3b5d8e31-0f6c-4949-8fb8-1895d9c48271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.reshape(1,9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r704ys1iz1BF",
        "outputId": "cbac4a69-ea41-4786-9c38-d4dd01775723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "GxcsGTW4z3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1,1,1,3,2,31,3,1,4]"
      ],
      "metadata": {
        "id": "0l8VK21cmFXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(Counter(x).values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQlCuUjBmKn0",
        "outputId": "7701e84e-49c1-4369-8689-8163954ae32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[2]*2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgMB51namL7w",
        "outputId": "c74979c3-c968-4318-fc5f-1c62fee3976b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tnsr1[0][1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fgdzO3yqN_4",
        "outputId": "b52a11c0-b1fa-4e85-eac9-b4f889ccf4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones_like(tnsr1[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVB-kkW0qYsH",
        "outputId": "82a23e45-42d3-489c-c8c4-ea63bb38e672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1],\n",
              "        [1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=torch.tensor([[[1,2],[2,2]]])\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtKOMV6Fsmbc",
        "outputId": "e76edf1e-72d9-4b16-c48d-012625fa482e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "label_mapping = {\n",
        "    'class1': 0,\n",
        "    'class2': 1,\n",
        "    'class3': 2\n",
        "}\n",
        "\n",
        "class customImageDataSet(Dataset):\n",
        "  def __init__(self, rootdir):\n",
        "    self.images = list(Path(rootdir).glob('*/*/*.jpg'))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    image = self.images[idx]\n",
        "    label = label_mapping.get(image.parent.name)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "0CmsGvFbtbBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh88S_O63gpl",
        "outputId": "802bb6c2-5036-4e76-d2f7-1aa044187948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "5ucKTiiv3pwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train = False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "NpKEASjb9ZhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}"
      ],
      "metadata": {
        "id": "B898Ucs298_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "#how to take a sample from dataloader\n",
        "\n",
        "x, y = next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "eHOv0YwS-3m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "print(nn.Flatten(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIx6jbETA9Ze",
        "outputId": "fdb33078-c326-40bd-ffd5-988f9a97ed50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flatten(\n",
            "  start_dim=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0039,  ..., 0.0078, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0039,  ..., 0.0078, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "  \n",
            "  \n",
            "          ...,\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            ...,\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), end_dim=-1\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 32),\n",
        "        nn.Relu(),\n",
        "        nn.Linear(32,16),\n",
        "        nn.Relu(),\n",
        "        nn.Linear(16,10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "VlSwmLIoBFbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "#2. Device Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "#3. Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "#4. Data Augmentation and normalization for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.2860), (0.3530))\n",
        "])\n",
        "\n",
        "#5. Load datasets\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#6. Define a neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.layer1 = nn.Linear(28*28, 128)\n",
        "    self.layer2 = nn.Linear(128, 64)\n",
        "    self.layer3 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28*28) #x.shape: (batch_size,1,28*28) -> (batch_size, 28*28). -1 infers to batch_size\n",
        "    x = torch.relu(self.layer1(x))\n",
        "    x = torch.relu(self.layer2(x))\n",
        "    x = self.layer3(x)\n",
        "    return x\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "#7. Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#8. Training the model\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels =  labels.to(device)\n",
        "\n",
        "    #forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    #backpropagation and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1)%100 == 0:\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], step [{i+1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "#9. Testing the model\n",
        "# predictions and labels\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outcomes = model(images)\n",
        "    _, predicted = torch.max(outcomes.data, 1)\n",
        "    total+=labels.size(0)\n",
        "    correct += (predicted==labels).sum().item()\n",
        "\n",
        "  print(f\"Test accuracy on the model of {total} test images is {(correct/total)*100:.2f}%\")\n",
        "\n",
        "\n",
        "#10. Training accuracy\n",
        "model.train()\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outcomes = model(images)\n",
        "    _, predicted = torch.max(outcomes.data, 1)\n",
        "    total+=labels.size(0)\n",
        "    correct += (predicted==labels).sum().item()\n",
        "    print('correct', correct)\n",
        "  print(f\"Test accuracy on the model of {total} training images is {(correct/total)*100}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuG8nUcBEWXc",
        "outputId": "abacd0f4-1bb3-4247-f52e-1c01e3dd2650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch [1/5], step [100/600], Loss: 0.6482\n",
            "Epoch [1/5], step [200/600], Loss: 0.4483\n",
            "Epoch [1/5], step [300/600], Loss: 0.4600\n",
            "Epoch [1/5], step [400/600], Loss: 0.4673\n",
            "Epoch [1/5], step [500/600], Loss: 0.3474\n",
            "Epoch [1/5], step [600/600], Loss: 0.4743\n",
            "Epoch [2/5], step [100/600], Loss: 0.2789\n",
            "Epoch [2/5], step [200/600], Loss: 0.3023\n",
            "Epoch [2/5], step [300/600], Loss: 0.3817\n",
            "Epoch [2/5], step [400/600], Loss: 0.3279\n",
            "Epoch [2/5], step [500/600], Loss: 0.3581\n",
            "Epoch [2/5], step [600/600], Loss: 0.3155\n",
            "Epoch [3/5], step [100/600], Loss: 0.3439\n",
            "Epoch [3/5], step [200/600], Loss: 0.4383\n",
            "Epoch [3/5], step [300/600], Loss: 0.3552\n",
            "Epoch [3/5], step [400/600], Loss: 0.3432\n",
            "Epoch [3/5], step [500/600], Loss: 0.3063\n",
            "Epoch [3/5], step [600/600], Loss: 0.4700\n",
            "Epoch [4/5], step [100/600], Loss: 0.3464\n",
            "Epoch [4/5], step [200/600], Loss: 0.2860\n",
            "Epoch [4/5], step [300/600], Loss: 0.2992\n",
            "Epoch [4/5], step [400/600], Loss: 0.1963\n",
            "Epoch [4/5], step [500/600], Loss: 0.2673\n",
            "Epoch [4/5], step [600/600], Loss: 0.2877\n",
            "Epoch [5/5], step [100/600], Loss: 0.2263\n",
            "Epoch [5/5], step [200/600], Loss: 0.2611\n",
            "Epoch [5/5], step [300/600], Loss: 0.2263\n",
            "Epoch [5/5], step [400/600], Loss: 0.2443\n",
            "Epoch [5/5], step [500/600], Loss: 0.2955\n",
            "Epoch [5/5], step [600/600], Loss: 0.3234\n",
            "Test accuracy on the model of 10000 test images is 87.70%\n",
            "correct 91\n",
            "correct 182\n",
            "correct 261\n",
            "correct 352\n",
            "correct 444\n",
            "correct 535\n",
            "correct 631\n",
            "correct 727\n",
            "correct 816\n",
            "correct 905\n",
            "correct 998\n",
            "correct 1093\n",
            "correct 1184\n",
            "correct 1276\n",
            "correct 1367\n",
            "correct 1454\n",
            "correct 1543\n",
            "correct 1633\n",
            "correct 1720\n",
            "correct 1807\n",
            "correct 1898\n",
            "correct 1991\n",
            "correct 2082\n",
            "correct 2168\n",
            "correct 2259\n",
            "correct 2349\n",
            "correct 2438\n",
            "correct 2532\n",
            "correct 2624\n",
            "correct 2718\n",
            "correct 2808\n",
            "correct 2902\n",
            "correct 2994\n",
            "correct 3077\n",
            "correct 3162\n",
            "correct 3255\n",
            "correct 3342\n",
            "correct 3434\n",
            "correct 3521\n",
            "correct 3609\n",
            "correct 3699\n",
            "correct 3793\n",
            "correct 3884\n",
            "correct 3973\n",
            "correct 4065\n",
            "correct 4153\n",
            "correct 4241\n",
            "correct 4330\n",
            "correct 4421\n",
            "correct 4510\n",
            "correct 4603\n",
            "correct 4690\n",
            "correct 4776\n",
            "correct 4868\n",
            "correct 4960\n",
            "correct 5054\n",
            "correct 5147\n",
            "correct 5236\n",
            "correct 5325\n",
            "correct 5411\n",
            "correct 5503\n",
            "correct 5591\n",
            "correct 5679\n",
            "correct 5771\n",
            "correct 5855\n",
            "correct 5944\n",
            "correct 6030\n",
            "correct 6116\n",
            "correct 6205\n",
            "correct 6298\n",
            "correct 6383\n",
            "correct 6472\n",
            "correct 6563\n",
            "correct 6654\n",
            "correct 6743\n",
            "correct 6833\n",
            "correct 6923\n",
            "correct 7014\n",
            "correct 7104\n",
            "correct 7197\n",
            "correct 7285\n",
            "correct 7375\n",
            "correct 7469\n",
            "correct 7563\n",
            "correct 7651\n",
            "correct 7739\n",
            "correct 7829\n",
            "correct 7920\n",
            "correct 8012\n",
            "correct 8102\n",
            "correct 8192\n",
            "correct 8283\n",
            "correct 8370\n",
            "correct 8464\n",
            "correct 8556\n",
            "correct 8647\n",
            "correct 8737\n",
            "correct 8830\n",
            "correct 8919\n",
            "correct 9012\n",
            "correct 9105\n",
            "correct 9195\n",
            "correct 9283\n",
            "correct 9379\n",
            "correct 9465\n",
            "correct 9557\n",
            "correct 9649\n",
            "correct 9739\n",
            "correct 9828\n",
            "correct 9915\n",
            "correct 10005\n",
            "correct 10100\n",
            "correct 10191\n",
            "correct 10280\n",
            "correct 10373\n",
            "correct 10468\n",
            "correct 10558\n",
            "correct 10645\n",
            "correct 10740\n",
            "correct 10829\n",
            "correct 10922\n",
            "correct 11013\n",
            "correct 11093\n",
            "correct 11183\n",
            "correct 11278\n",
            "correct 11367\n",
            "correct 11459\n",
            "correct 11551\n",
            "correct 11642\n",
            "correct 11727\n",
            "correct 11808\n",
            "correct 11903\n",
            "correct 11997\n",
            "correct 12088\n",
            "correct 12185\n",
            "correct 12277\n",
            "correct 12371\n",
            "correct 12462\n",
            "correct 12554\n",
            "correct 12642\n",
            "correct 12733\n",
            "correct 12823\n",
            "correct 12918\n",
            "correct 13007\n",
            "correct 13095\n",
            "correct 13184\n",
            "correct 13272\n",
            "correct 13356\n",
            "correct 13448\n",
            "correct 13538\n",
            "correct 13625\n",
            "correct 13720\n",
            "correct 13813\n",
            "correct 13902\n",
            "correct 13993\n",
            "correct 14080\n",
            "correct 14169\n",
            "correct 14258\n",
            "correct 14344\n",
            "correct 14434\n",
            "correct 14525\n",
            "correct 14610\n",
            "correct 14700\n",
            "correct 14790\n",
            "correct 14878\n",
            "correct 14967\n",
            "correct 15053\n",
            "correct 15142\n",
            "correct 15224\n",
            "correct 15311\n",
            "correct 15400\n",
            "correct 15487\n",
            "correct 15578\n",
            "correct 15672\n",
            "correct 15763\n",
            "correct 15850\n",
            "correct 15944\n",
            "correct 16031\n",
            "correct 16116\n",
            "correct 16206\n",
            "correct 16297\n",
            "correct 16387\n",
            "correct 16480\n",
            "correct 16564\n",
            "correct 16649\n",
            "correct 16737\n",
            "correct 16826\n",
            "correct 16917\n",
            "correct 17007\n",
            "correct 17097\n",
            "correct 17189\n",
            "correct 17273\n",
            "correct 17363\n",
            "correct 17453\n",
            "correct 17541\n",
            "correct 17625\n",
            "correct 17713\n",
            "correct 17800\n",
            "correct 17893\n",
            "correct 17982\n",
            "correct 18068\n",
            "correct 18157\n",
            "correct 18251\n",
            "correct 18345\n",
            "correct 18436\n",
            "correct 18524\n",
            "correct 18616\n",
            "correct 18703\n",
            "correct 18795\n",
            "correct 18885\n",
            "correct 18971\n",
            "correct 19061\n",
            "correct 19153\n",
            "correct 19244\n",
            "correct 19337\n",
            "correct 19424\n",
            "correct 19514\n",
            "correct 19609\n",
            "correct 19699\n",
            "correct 19793\n",
            "correct 19887\n",
            "correct 19974\n",
            "correct 20068\n",
            "correct 20162\n",
            "correct 20254\n",
            "correct 20345\n",
            "correct 20436\n",
            "correct 20526\n",
            "correct 20618\n",
            "correct 20707\n",
            "correct 20795\n",
            "correct 20882\n",
            "correct 20974\n",
            "correct 21067\n",
            "correct 21159\n",
            "correct 21250\n",
            "correct 21340\n",
            "correct 21434\n",
            "correct 21520\n",
            "correct 21615\n",
            "correct 21705\n",
            "correct 21793\n",
            "correct 21883\n",
            "correct 21971\n",
            "correct 22060\n",
            "correct 22144\n",
            "correct 22233\n",
            "correct 22318\n",
            "correct 22411\n",
            "correct 22503\n",
            "correct 22595\n",
            "correct 22689\n",
            "correct 22783\n",
            "correct 22874\n",
            "correct 22966\n",
            "correct 23057\n",
            "correct 23149\n",
            "correct 23242\n",
            "correct 23331\n",
            "correct 23420\n",
            "correct 23508\n",
            "correct 23600\n",
            "correct 23690\n",
            "correct 23780\n",
            "correct 23874\n",
            "correct 23965\n",
            "correct 24054\n",
            "correct 24144\n",
            "correct 24233\n",
            "correct 24327\n",
            "correct 24415\n",
            "correct 24506\n",
            "correct 24602\n",
            "correct 24691\n",
            "correct 24777\n",
            "correct 24872\n",
            "correct 24963\n",
            "correct 25051\n",
            "correct 25142\n",
            "correct 25236\n",
            "correct 25327\n",
            "correct 25417\n",
            "correct 25505\n",
            "correct 25595\n",
            "correct 25688\n",
            "correct 25780\n",
            "correct 25872\n",
            "correct 25962\n",
            "correct 26053\n",
            "correct 26143\n",
            "correct 26233\n",
            "correct 26322\n",
            "correct 26409\n",
            "correct 26496\n",
            "correct 26585\n",
            "correct 26679\n",
            "correct 26770\n",
            "correct 26862\n",
            "correct 26948\n",
            "correct 27038\n",
            "correct 27129\n",
            "correct 27221\n",
            "correct 27311\n",
            "correct 27393\n",
            "correct 27485\n",
            "correct 27576\n",
            "correct 27663\n",
            "correct 27750\n",
            "correct 27843\n",
            "correct 27932\n",
            "correct 28021\n",
            "correct 28106\n",
            "correct 28202\n",
            "correct 28286\n",
            "correct 28372\n",
            "correct 28465\n",
            "correct 28556\n",
            "correct 28644\n",
            "correct 28735\n",
            "correct 28824\n",
            "correct 28915\n",
            "correct 29007\n",
            "correct 29095\n",
            "correct 29181\n",
            "correct 29272\n",
            "correct 29359\n",
            "correct 29449\n",
            "correct 29540\n",
            "correct 29629\n",
            "correct 29720\n",
            "correct 29811\n",
            "correct 29902\n",
            "correct 29995\n",
            "correct 30084\n",
            "correct 30175\n",
            "correct 30258\n",
            "correct 30353\n",
            "correct 30442\n",
            "correct 30533\n",
            "correct 30627\n",
            "correct 30717\n",
            "correct 30811\n",
            "correct 30898\n",
            "correct 30987\n",
            "correct 31076\n",
            "correct 31158\n",
            "correct 31250\n",
            "correct 31346\n",
            "correct 31431\n",
            "correct 31521\n",
            "correct 31607\n",
            "correct 31701\n",
            "correct 31789\n",
            "correct 31883\n",
            "correct 31970\n",
            "correct 32060\n",
            "correct 32144\n",
            "correct 32230\n",
            "correct 32325\n",
            "correct 32417\n",
            "correct 32509\n",
            "correct 32601\n",
            "correct 32690\n",
            "correct 32785\n",
            "correct 32876\n",
            "correct 32967\n",
            "correct 33055\n",
            "correct 33145\n",
            "correct 33234\n",
            "correct 33327\n",
            "correct 33413\n",
            "correct 33497\n",
            "correct 33585\n",
            "correct 33675\n",
            "correct 33766\n",
            "correct 33860\n",
            "correct 33948\n",
            "correct 34039\n",
            "correct 34128\n",
            "correct 34216\n",
            "correct 34308\n",
            "correct 34396\n",
            "correct 34488\n",
            "correct 34580\n",
            "correct 34673\n",
            "correct 34763\n",
            "correct 34856\n",
            "correct 34943\n",
            "correct 35034\n",
            "correct 35123\n",
            "correct 35210\n",
            "correct 35298\n",
            "correct 35390\n",
            "correct 35480\n",
            "correct 35570\n",
            "correct 35658\n",
            "correct 35751\n",
            "correct 35841\n",
            "correct 35931\n",
            "correct 36023\n",
            "correct 36117\n",
            "correct 36206\n",
            "correct 36298\n",
            "correct 36388\n",
            "correct 36480\n",
            "correct 36568\n",
            "correct 36660\n",
            "correct 36748\n",
            "correct 36837\n",
            "correct 36923\n",
            "correct 37015\n",
            "correct 37109\n",
            "correct 37203\n",
            "correct 37293\n",
            "correct 37379\n",
            "correct 37472\n",
            "correct 37560\n",
            "correct 37652\n",
            "correct 37734\n",
            "correct 37821\n",
            "correct 37909\n",
            "correct 38003\n",
            "correct 38094\n",
            "correct 38182\n",
            "correct 38271\n",
            "correct 38359\n",
            "correct 38444\n",
            "correct 38533\n",
            "correct 38627\n",
            "correct 38719\n",
            "correct 38812\n",
            "correct 38905\n",
            "correct 38992\n",
            "correct 39082\n",
            "correct 39173\n",
            "correct 39263\n",
            "correct 39352\n",
            "correct 39442\n",
            "correct 39535\n",
            "correct 39624\n",
            "correct 39715\n",
            "correct 39808\n",
            "correct 39901\n",
            "correct 39988\n",
            "correct 40083\n",
            "correct 40173\n",
            "correct 40269\n",
            "correct 40357\n",
            "correct 40446\n",
            "correct 40539\n",
            "correct 40629\n",
            "correct 40721\n",
            "correct 40814\n",
            "correct 40905\n",
            "correct 40991\n",
            "correct 41081\n",
            "correct 41168\n",
            "correct 41259\n",
            "correct 41348\n",
            "correct 41436\n",
            "correct 41524\n",
            "correct 41617\n",
            "correct 41704\n",
            "correct 41797\n",
            "correct 41888\n",
            "correct 41976\n",
            "correct 42064\n",
            "correct 42160\n",
            "correct 42250\n",
            "correct 42343\n",
            "correct 42431\n",
            "correct 42526\n",
            "correct 42620\n",
            "correct 42712\n",
            "correct 42803\n",
            "correct 42896\n",
            "correct 42984\n",
            "correct 43071\n",
            "correct 43160\n",
            "correct 43249\n",
            "correct 43338\n",
            "correct 43432\n",
            "correct 43521\n",
            "correct 43612\n",
            "correct 43704\n",
            "correct 43791\n",
            "correct 43878\n",
            "correct 43960\n",
            "correct 44048\n",
            "correct 44139\n",
            "correct 44227\n",
            "correct 44317\n",
            "correct 44407\n",
            "correct 44496\n",
            "correct 44587\n",
            "correct 44682\n",
            "correct 44772\n",
            "correct 44862\n",
            "correct 44953\n",
            "correct 45045\n",
            "correct 45135\n",
            "correct 45222\n",
            "correct 45314\n",
            "correct 45407\n",
            "correct 45494\n",
            "correct 45582\n",
            "correct 45669\n",
            "correct 45759\n",
            "correct 45852\n",
            "correct 45941\n",
            "correct 46028\n",
            "correct 46121\n",
            "correct 46211\n",
            "correct 46300\n",
            "correct 46391\n",
            "correct 46482\n",
            "correct 46573\n",
            "correct 46660\n",
            "correct 46745\n",
            "correct 46832\n",
            "correct 46925\n",
            "correct 47020\n",
            "correct 47107\n",
            "correct 47198\n",
            "correct 47291\n",
            "correct 47383\n",
            "correct 47469\n",
            "correct 47556\n",
            "correct 47642\n",
            "correct 47728\n",
            "correct 47819\n",
            "correct 47911\n",
            "correct 48002\n",
            "correct 48092\n",
            "correct 48181\n",
            "correct 48269\n",
            "correct 48359\n",
            "correct 48448\n",
            "correct 48544\n",
            "correct 48639\n",
            "correct 48730\n",
            "correct 48824\n",
            "correct 48913\n",
            "correct 49001\n",
            "correct 49092\n",
            "correct 49184\n",
            "correct 49269\n",
            "correct 49359\n",
            "correct 49447\n",
            "correct 49531\n",
            "correct 49619\n",
            "correct 49712\n",
            "correct 49805\n",
            "correct 49895\n",
            "correct 49988\n",
            "correct 50078\n",
            "correct 50167\n",
            "correct 50260\n",
            "correct 50351\n",
            "correct 50445\n",
            "correct 50535\n",
            "correct 50629\n",
            "correct 50718\n",
            "correct 50804\n",
            "correct 50895\n",
            "correct 50986\n",
            "correct 51078\n",
            "correct 51171\n",
            "correct 51257\n",
            "correct 51345\n",
            "correct 51437\n",
            "correct 51528\n",
            "correct 51616\n",
            "correct 51700\n",
            "correct 51796\n",
            "correct 51886\n",
            "correct 51979\n",
            "correct 52072\n",
            "correct 52163\n",
            "correct 52252\n",
            "correct 52339\n",
            "correct 52428\n",
            "correct 52518\n",
            "correct 52605\n",
            "correct 52687\n",
            "correct 52776\n",
            "correct 52865\n",
            "correct 52958\n",
            "correct 53048\n",
            "correct 53138\n",
            "correct 53225\n",
            "correct 53315\n",
            "correct 53403\n",
            "correct 53495\n",
            "correct 53584\n",
            "correct 53676\n",
            "correct 53762\n",
            "correct 53851\n",
            "correct 53940\n",
            "correct 54034\n",
            "Test accuracy on the model of 60000 training images is 90.05666666666666%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=next(iter(test_loader))"
      ],
      "metadata": {
        "id": "qUl2bOBKT0oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size(0)"
      ],
      "metadata": {
        "id": "efOaOW5uUocG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymPUf6y6nFh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}